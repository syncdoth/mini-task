{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":2,"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"similarity approach.ipynb","provenance":[{"file_id":"1MIwisL0hky3Dj7whAiDAndLT5SgtPej7","timestamp":1610388342264},{"file_id":"1cu2Etbv3df-KXmYaAbcBMoDjvTojpDHQ","timestamp":1610388310660},{"file_id":"15hzLzh2_NHRL7bY_iq1VXfVqgI41OYNG","timestamp":1610382620530}],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IBWJRscyoAL0","executionInfo":{"status":"ok","timestamp":1610629748232,"user_tz":-540,"elapsed":24445,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}},"outputId":"ab3b596e-8b74-413f-8352-d5ea9a29787a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yKk4AtCrn2Qb","executionInfo":{"status":"ok","timestamp":1610629730463,"user_tz":-540,"elapsed":6695,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}},"outputId":"68f2fe2a-9e89-4b3b-ff93-ab5338c973c0"},"source":["import os\n","import re\n","from itertools import chain \n","\n","import nltk\n","from nltk.stem.porter import PorterStemmer\n","from nltk.corpus import wordnet as wn\n","\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow import keras\n","import tensorflow as tf\n","from scipy import sparse\n","import numpy as np"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"tags":[],"id":"gxALCsFpn2Qi","executionInfo":{"status":"ok","timestamp":1610629730464,"user_tz":-540,"elapsed":6692,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}}},"source":["def get_bow(data, encoder):\n","    bows = []\n","    for ex in data:\n","        encoded = [encoder.get(w, 1) for w in ex]\n","        x = to_categorical(encoded, num_classes=len(encoder))\n","        x = np.sum(x, axis=0)\n","        bows.append(x)\n","    return np.array(bows)\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"tYPJV2_9GIJQ","executionInfo":{"status":"ok","timestamp":1610629730464,"user_tz":-540,"elapsed":6689,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}}},"source":["def is_noun(word, tag):\n","    return (tag == \"NN\" or tag == \"NNS\") and word.isalpha() and len(word) > 1\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"S0EoyVlvMcGb","executionInfo":{"status":"ok","timestamp":1610629730465,"user_tz":-540,"elapsed":6688,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}}},"source":["def get_wordnet_repr(word):\n","    synset = wn.synsets(word, \"n\")\n","    if len(synset) > 0:\n","        return synset[0]\n","    else:\n","        return None"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"kNrKo0piG59U","executionInfo":{"status":"ok","timestamp":1610629730466,"user_tz":-540,"elapsed":6687,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}}},"source":["def preprocess_sentence(sent):\n","    sent = sent.strip()\n","    # delete unwanted special characters\n","    sent = re.sub(r\"[@#\\^\\*\\(\\)\\\\\\|~;\\\"=+`]\", \"\", sent)\n","\n","    # handle some special characters\n","    sent = sent.replace(\"$\", \" dollar \")\n","    sent = sent.replace(\"%\", \" percent \")\n","    sent = sent.replace(\"&\", \" and \")\n","    sent = re.sub(\"[-_:]\", \" \", sent)\n","    sent = sent.lower()\n","    \n","    return sent"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"mSmG-94NptM9","executionInfo":{"status":"ok","timestamp":1610632606425,"user_tz":-540,"elapsed":1249,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}}},"source":["class Dataset:\n","    def __init__(self, mode=\"train\", stem_func=None, tag_func=nltk.pos_tag):\n","        if stem_func is not None:\n","            self.stem_func = stem_func\n","        else:\n","            self.stem_func = lambda x: x\n","\n","        self.tag_func = tag_func\n","        self.mode = mode\n","\n","        self.word2idx = None\n","        self.idx2word = None\n","        self.similarity_matrix = None\n","\n","    def get_corpus(self):\n","        \"\"\"List of Dialogues. Each dialogue is list of sentences (question and answer \n","        are separate sentences). Each sentece is tokenized.\n","        \"\"\"\n","        corpus = []\n","        with open(f\"drive/MyDrive/task/data/dialog_{self.mode}.txt\") as f:\n","            for line in f:\n","                line = preprocess_sentence(line)\n","\n","                # simple heuristic to avoid mistake in pos tagging\n","                line = line.replace(\"yes\", \"Yes\")\n","\n","                line = line.replace(\"</q>\", \"?\")\n","                line = line.replace(\"</a>\", \".\")\n","\n","                # split by sentence\n","                line = re.split(\"<[qa]>\", line)\n","                # drop empty strings created from split, and tokenize\n","                line = [nltk.word_tokenize(s) for s in line if s]\n","                corpus.append(line)\n","\n","        print(\"finished loading corpus\")\n","        return corpus\n","\n","    def get_description(self):\n","        \"\"\"List of descriptions. The descriptions are tokenized.\"\"\"\n","        with open(f\"drive/MyDrive/task/data/desc_{self.mode}.txt\") as f:\n","            descriptions = []\n","            for line in f:\n","                line = preprocess_sentence(line)\n","                line = nltk.word_tokenize(line)\n","                descriptions.append(line)\n","\n","        print(\"finished loading descriptions\")\n","        return descriptions\n","\n","    def extract_nouns(self, tagged_sentence):\n","        nouns = []\n","        for w in tagged_sentence:\n","            if not is_noun(*w):\n","                continue\n","            synset = get_wordnet_repr(w[0])\n","            if synset is None:\n","                continue\n","            nouns.append(synset.name())\n","        return nouns\n","\n","\n","    def extract_context(self, corpus):\n","        \"\"\"Context from dialogues. \n","        shape: [number of dialogue, number of context]\"\"\"\n","        contexts = []\n","        for i, dialog in enumerate(corpus):\n","            tagged_dialog = self.tag_func(dialog)\n","            context = set()\n","            for sent in tagged_dialog:\n","                nouns = self.extract_nouns(sent)\n","                context.update(nouns)\n","            contexts.append(list(context))\n","\n","        print(\"finished extracting contexts\")\n","        return contexts\n","    \n","    def extract_target(self, description, contexts):\n","        \"\"\"Extract ground truth OOC.\n","\n","        NOTE: There are quite a lot of instances there is no OOC.\n","        \"\"\"\n","        targets = []\n","        tagged_description = self.tag_func(description)\n","        for i, desc in enumerate(tagged_description):\n","            nouns = self.extract_nouns(desc)\n","            ooc = set(nouns) - set(contexts[i])\n","            targets.append(list(ooc))\n","\n","        print(\"finished extracting targets (OOCs)\")\n","        return targets\n","    \n","    def set_vocab(self, context, target):\n","        \"\"\"Vocabulary of the dataset.\"\"\"\n","        vocab = set()\n","        vocab.update(chain.from_iterable(context))\n","        vocab.update(chain.from_iterable(target))\n","\n","        self.word2idx = {word: idx + 2 for idx, word in enumerate(vocab)}\n","        self.word2idx[\"[pad]\"] = 0\n","        self.word2idx[\"[unk]\"] = 1\n","\n","        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n","    \n","\n","    def set_similarity_matrix(self):\n","        self.similarity_matrix = sparse.lil_matrix((len(self.word2idx), len(self.word2idx)))  # [V, V]\n","\n","        for i, word1 in self.idx2word.items():\n","            if i in [0, 1]:\n","                continue\n","\n","            w1 = wn.synset(word1)\n","            for j, word2 in self.idx2word.items():\n","                if j in [0, 1]:\n","                    continue\n","                if j == i:\n","                    self.similarity_matrix[i, j] = -1\n","                    continue\n","\n","                w2 = wn.synset(word2)\n","                similarity = w1.wup_similarity(w2)\n","                if similarity is None:\n","                    similarity = 0\n","\n","                self.similarity_matrix[i, j] = similarity\n","            if i % 100 == 0:\n","                print(i)\n","        self.similarity_matrix = self.similarity_matrix.tocsr()\n","\n","\n","        print(\"finished setting up similarity matrix\")\n","\n","\n","\n","    def get_data_target(self, word2idx=None, sim_mat=None):\n","        corpus = self.get_corpus()  #[N, D, t]\n","        description = self.get_description()  #[N, D]\n","\n","        contexts = self.extract_context(corpus)  # [N, t]\n","        target = self.extract_target(description, contexts)  # [N, G]\n","\n","        if word2idx is None:\n","            self.set_vocab(contexts, target)\n","            self.set_similarity_matrix()\n","            word2idx = self.word2idx\n","            sim_mat = self.similarity_matrix\n","\n","        # TODO: change this part\n","        data_idx = [[word2idx.get(tok, 1) for tok in dialog] for dialog in contexts]  # [N, t]\n","        data = [sim_mat[idx].sum(0) for idx in data_idx]  # [N, V]\n","        # maxlen = np.max([ex.shape[0] for ex in data])\n","        # data = [csr_matrix((ex.data, ex.indices, np.pad(ex.indptr, (0, maxlen - ex.shape[0]))) for ex in data]\n","        data = np.vstack(data)\n","        print(data.shape)\n","\n","        target_label = get_bow(target, word2idx)  # [N, V]\n","\n","        defined_idx = np.where(target_label.sum(1) > 0)[0]\n","\n","        return data[defined_idx], target_label[defined_idx]"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"mxkXiCpHqIV8","executionInfo":{"status":"ok","timestamp":1610641423419,"user_tz":-540,"elapsed":2133,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}}},"source":["from keras.preprocessing.text import Tokenizer\n","tokenizer = Tokenizer()\n","\n","tokenizer.fit_on_texts([list(chain.from_iterable(dialog)) for dialog in train_corpus])"],"execution_count":264,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":456},"id":"k2e48WfFqU75","executionInfo":{"status":"error","timestamp":1610642505395,"user_tz":-540,"elapsed":1374,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}},"outputId":"57220fb2-6fbc-46e7-966b-f5cf4dcc88ef"},"source":["x = tokenizer.texts_to_sequences(train_corpus[0])\n","x = np.expand_dims(x, axis=0)\n","to_categorical(list(chain.from_iterable(x)), len(tokenizer.word_counts)+1)"],"execution_count":278,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order, subok=True)\n"],"name":"stderr"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-278-cd43c7cff61a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes, dtype)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   \"\"\"\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9sOryDlPslcr","executionInfo":{"status":"ok","timestamp":1610641268078,"user_tz":-540,"elapsed":1007,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}},"outputId":"6df59f70-543d-4297-efd3-0106662bbc88"},"source":["len(train_corpus[0]"],"execution_count":252,"outputs":[{"output_type":"execute_result","data":{"text/plain":["20"]},"metadata":{"tags":[]},"execution_count":252}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H3v5TrGG_LAm","executionInfo":{"status":"ok","timestamp":1610632606426,"user_tz":-540,"elapsed":1115,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}},"outputId":"9ddb2e8e-ef0f-433e-d731-73a57885c440"},"source":["from nltk.tag import StanfordPOSTagger\n","\n","STANFORD_POS_MODEL_PATH = \"/content/drive/MyDrive/stanford-postagger-full-2020-11-17/models/english-bidirectional-distsim.tagger\"\n","STANFORD_POS_JAR_PATH = \"/content/drive/MyDrive/stanford-postagger-full-2020-11-17/stanford-postagger-4.2.0.jar\"\n","\n","pos_tagger = StanfordPOSTagger(STANFORD_POS_MODEL_PATH, STANFORD_POS_JAR_PATH)\n","\n","lemma = nltk.wordnet.WordNetLemmatizer()"],"execution_count":51,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/nltk/tag/stanford.py:149: DeprecationWarning: \n","The StanfordTokenizer will be deprecated in version 3.2.5.\n","Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n","  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"7FOHzZ-Gn2Qk","executionInfo":{"status":"ok","timestamp":1610632606426,"user_tz":-540,"elapsed":956,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}}},"source":["train_ds = Dataset(mode=\"train\", stem_func=lemma.lemmatize, tag_func=nltk.pos_tag_sents)\n","val_ds = Dataset(mode=\"valid\", stem_func=lemma.lemmatize, tag_func=nltk.pos_tag_sents)"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SUJxcACXAptg","executionInfo":{"status":"ok","timestamp":1610632627239,"user_tz":-540,"elapsed":21145,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}},"outputId":"68f8e329-e55b-4191-bbc0-a3bec4dc8a21"},"source":["train_corpus = train_ds.get_corpus()\n","train_description = train_ds.get_description()"],"execution_count":53,"outputs":[{"output_type":"stream","text":["finished loading corpus\n","finished loading descriptions\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nuCgYM9dB0_0","executionInfo":{"status":"ok","timestamp":1610632669669,"user_tz":-540,"elapsed":62997,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}},"outputId":"e6a0c5f1-2765-46d8-8f61-0cbabd8fb73a"},"source":["train_context = train_ds.extract_context(train_corpus)\n","train_target = train_ds.extract_target(train_description, train_context)"],"execution_count":54,"outputs":[{"output_type":"stream","text":["finished extracting contexts\n","finished extracting targets (OOCs)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JIFfJXfSL1vm"},"source":["## Debug"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jv2HdEbyKNjn","executionInfo":{"status":"ok","timestamp":1610632239294,"user_tz":-540,"elapsed":1293,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}},"outputId":"f0d9c5c0-ac45-4bf3-9af8-37bd2eec386d"},"source":["x = zip(train_context, train_target)\n","next(x)"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['grey.n.01',\n","  'bus.n.01',\n","  'ad.n.01',\n","  'photograph.n.01',\n","  'top.n.01',\n","  'woman.n.01',\n","  'color.n.01',\n","  'yoga.n.01',\n","  'bloomers.n.01'],\n"," ['tour.n.01', 'business_district.n.01'])"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OMGI6YjVKaG_","executionInfo":{"status":"ok","timestamp":1610632341006,"user_tz":-540,"elapsed":1571,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}},"outputId":"2a0b0bd7-4850-4052-e498-7a5a06a0cf97"},"source":["next(x)"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['hair.n.01',\n","  'man.n.01',\n","  'clean_and_jerk.n.01',\n","  'shirt.n.01',\n","  'color.n.01',\n","  'position.n.03',\n","  'wood.n.01',\n","  'laptop.n.01',\n","  'taste.n.01',\n","  'wear.n.01',\n","  'spectacles.n.01',\n","  'screen.n.01',\n","  'beard.n.01',\n","  'model.n.01',\n","  'brown.n.01',\n","  'table.n.01'],\n"," ['work.n.01', 'people.n.01'])"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"sUAigg1TL-U5"},"source":["## Again"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wnGLh2x9BWJ4","executionInfo":{"status":"ok","timestamp":1610636027229,"user_tz":-540,"elapsed":3282577,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}},"outputId":"c12c4ebe-900c-492a-8d21-1d079979f0b0"},"source":["train_ds.set_vocab(train_context, train_target)\n","print(\"vocab_size:\", len(train_ds.word2idx))\n","\n","train_ds.set_similarity_matrix()"],"execution_count":56,"outputs":[{"output_type":"stream","text":["vocab_size: 4263\n","100\n","200\n","300\n","400\n","500\n","600\n","700\n","800\n","900\n","1000\n","1100\n","1200\n","1300\n","1400\n","1500\n","1600\n","1700\n","1800\n","1900\n","2000\n","2100\n","2200\n","2300\n","2400\n","2500\n","2600\n","2700\n","2800\n","2900\n","3000\n","3100\n","3200\n","3300\n","3400\n","3500\n","3600\n","3700\n","3800\n","3900\n","4000\n","4100\n","4200\n","finished setting up similarity matrix\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-hIbLkCBV_sq","executionInfo":{"status":"ok","timestamp":1610638084469,"user_tz":-540,"elapsed":8730,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}},"outputId":"a85906cc-97bb-4a53-ba7d-c3b91ac0ac1c"},"source":["# TODO: change this part\n","data_idx = [[train_ds.word2idx.get(tok, 1) for tok in dialog] for dialog in train_context]  # [N, t]\n","train_data = [train_ds.similarity_matrix[idx].sum(axis=0) for idx in data_idx]  # [N, V]\n","train_data = np.vstack(train_data)\n","print(train_data.shape)\n","\n","train_target_label = get_bow(train_target, train_ds.word2idx)  # [N, V]\n","\n","defined_idx = np.where(train_target_label.sum(1) > 0)[0]\n","\n","train_data, train_target_label = train_data[defined_idx], train_target_label[defined_idx]"],"execution_count":151,"outputs":[{"output_type":"stream","text":["(9666, 4263)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wJXwJkeEkEhj","executionInfo":{"status":"ok","timestamp":1610636703019,"user_tz":-540,"elapsed":12197,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}},"outputId":"db6fff16-22d5-45c7-e431-61b1b7e39321"},"source":["val_data, val_target = val_ds.get_data_target(train_ds.word2idx, train_ds.similarity_matrix)"],"execution_count":76,"outputs":[{"output_type":"stream","text":["finished loading corpus\n","finished loading descriptions\n","finished extracting contexts\n","finished extracting targets (OOCs)\n","(1208, 4263)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sL7kNlMlgfQs","executionInfo":{"status":"ok","timestamp":1610638416196,"user_tz":-540,"elapsed":1013,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}}},"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","scaler = MinMaxScaler()\n","train_data = scaler.fit_transform(train_data)"],"execution_count":173,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iua_B4-nh-yZ","executionInfo":{"status":"ok","timestamp":1610638424364,"user_tz":-540,"elapsed":1312,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}},"outputId":"9a6a7c69-2bb2-46a0-cfb0-7bb77f0cbe93"},"source":["train_data"],"execution_count":174,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.        , 0.        , 0.20718095, ..., 0.21807308, 0.2448492 ,\n","        0.17003071],\n","       [0.        , 0.        , 0.34301971, ..., 0.37115245, 0.39008822,\n","        0.30458703],\n","       [0.        , 0.        , 0.31039884, ..., 0.4722585 , 0.34761265,\n","        0.26523564],\n","       ...,\n","       [0.        , 0.        , 0.18723287, ..., 0.38439629, 0.24057819,\n","        0.16607384],\n","       [0.        , 0.        , 0.35053555, ..., 0.42531133, 0.36530319,\n","        0.281625  ],\n","       [0.        , 0.        , 0.28140507, ..., 0.47912925, 0.33149306,\n","        0.25030169]])"]},"metadata":{"tags":[]},"execution_count":174}]},{"cell_type":"markdown","metadata":{"id":"cWUKN1YWcOST"},"source":["## Save"]},{"cell_type":"code","metadata":{"id":"InxmlMtHJVMq","executionInfo":{"status":"ok","timestamp":1610636034967,"user_tz":-540,"elapsed":7724,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}}},"source":["with open(\"/content/drive/MyDrive/word2idx.txt\", \"w\") as f:\n","    for k, v in train_ds.word2idx.items():\n","        f.write(f\"{k}, {v}\\n\")\n","\n","sparse.save_npz(\"/content/drive/MyDrive/sim_mat\", train_ds.similarity_matrix)"],"execution_count":57,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iRjwKrVsgxJC"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"_MZHYa-9n2Ql","executionInfo":{"status":"ok","timestamp":1610638434483,"user_tz":-540,"elapsed":1139,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}}},"source":["x = keras.layers.Input(shape=(train_data.shape[1],))  # [N, V]\n","\n","h = keras.layers.Dense(512, activation=\"relu\")(x)\n","h = keras.layers.Dense(1024, activation=\"relu\")(h)\n","h = keras.layers.Dense(2048, activation=\"relu\")(h)\n","y = keras.layers.Dense(train_target_label.shape[-1], activation=\"sigmoid\")(h)\n","\n","model = keras.models.Model(x, y)\n","\n","model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-5), loss=keras.losses.BinaryCrossentropy(), metrics=['accuracy'])"],"execution_count":175,"outputs":[]},{"cell_type":"code","metadata":{"id":"klzClaUkn2Ql","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610638494758,"user_tz":-540,"elapsed":59623,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}},"outputId":"e1bd45d3-f3bf-4863-a3fc-fbbd3fc0573b"},"source":["model.fit(train_data, train_target_label, batch_size=16, epochs=10) # validation_data=(val_data, val_target)"],"execution_count":176,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","539/539 [==============================] - 6s 11ms/step - loss: 0.2965 - accuracy: 0.0062\n","Epoch 2/10\n","539/539 [==============================] - 6s 11ms/step - loss: 0.0050 - accuracy: 0.0227\n","Epoch 3/10\n","539/539 [==============================] - 6s 11ms/step - loss: 0.0046 - accuracy: 0.0299\n","Epoch 4/10\n","539/539 [==============================] - 6s 11ms/step - loss: 0.0044 - accuracy: 0.0356\n","Epoch 5/10\n","539/539 [==============================] - 6s 11ms/step - loss: 0.0044 - accuracy: 0.0243\n","Epoch 6/10\n","539/539 [==============================] - 6s 11ms/step - loss: 0.0044 - accuracy: 0.0305\n","Epoch 7/10\n","539/539 [==============================] - 6s 11ms/step - loss: 0.0044 - accuracy: 0.0277\n","Epoch 8/10\n","539/539 [==============================] - 6s 11ms/step - loss: 0.0043 - accuracy: 0.0279\n","Epoch 9/10\n","539/539 [==============================] - 6s 11ms/step - loss: 0.0043 - accuracy: 0.0299\n","Epoch 10/10\n","539/539 [==============================] - 6s 11ms/step - loss: 0.0041 - accuracy: 0.0295\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f66d6777550>"]},"metadata":{"tags":[]},"execution_count":176}]},{"cell_type":"markdown","metadata":{"id":"GAnTW2lWqpHP"},"source":["## Predictions"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qpIoGYnBgAD-","executionInfo":{"status":"ok","timestamp":1610638498438,"user_tz":-540,"elapsed":1143,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}},"outputId":"9bfbc674-e98c-45be-958d-5c2e1b62d2ab"},"source":["train_data[800]"],"execution_count":177,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.        , 0.        , 0.14685103, ..., 0.17356423, 0.1892548 ,\n","       0.11852542])"]},"metadata":{"tags":[]},"execution_count":177}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8wkkfAaEo1Ac","executionInfo":{"status":"ok","timestamp":1610639899482,"user_tz":-540,"elapsed":1176,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}},"outputId":"cb38d5bd-09c9-426a-a8d3-b76768545659"},"source":["x = 1000\n","# pred = model(np.expand_dims(train_data[x], axis=0)).numpy()\n","# top10 = pred.flatten().argsort()[-20:][::-1]\n","top10 = np.array(train_data)[x].argsort()[::-1][:10]\n","\n","print(\"pred\\n\")\n","for i in top10.flatten():\n","    # print(train_ds.idx2word[i], pred[0, i])\n","    print(train_ds.idx2word[i])\n","\n","print(\"\\ntarget\\n\")\n","for i in np.where(train_target_label[x] == 1)[0]:\n","    print(train_ds.idx2word[i])\n","\n","# print(\"\\ninput\\n\")\n","# for i in train_contexts[x]:\n","#     print(i)"],"execution_count":230,"outputs":[{"output_type":"stream","text":["pred\n","\n","moped.n.01\n","sport_utility.n.01\n","cab.n.03\n","minivan.n.01\n","hatchback.n.01\n","ambulance.n.01\n","jeep.n.01\n","limousine.n.01\n","convertible.n.01\n","sedan.n.01\n","\n","target\n","\n","back.n.01\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u1DKoBEcfAkJ","executionInfo":{"status":"ok","timestamp":1610639763964,"user_tz":-540,"elapsed":1517,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}},"outputId":"35ef818e-792b-4638-b731-b5e1bd3e21f5"},"source":["from nltk.wsd import lesk\n","\n","lesk(\"a small glass pitcher sitting on a table with a flower in it\", \"pitcher\", \"n\")\n","wn.synsets(\"pitcher\")[2].definition()\n","wn.synset(\"home_plate.n.01\").lemma_names()"],"execution_count":226,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['home_plate', 'home_base', 'home', 'plate']"]},"metadata":{"tags":[]},"execution_count":226}]},{"cell_type":"code","metadata":{"id":"G6TJTh1tILDE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610636872051,"user_tz":-540,"elapsed":1197,"user":{"displayName":"sehyun choi","photoUrl":"","userId":"03405904696769351234"}},"outputId":"c439f33d-da46-4bad-8463-c8de20801054"},"source":["x = 12\n","pred = model(val_data[x]).numpy()\n","top10 = pred.flatten().argsort()[-10:][::-1]\n","# top10 = np.array(train_data)[x].argsort()[::-1][:10]\n","\n","print(\"pred\\n\")\n","for i in top10.flatten():\n","    # print(train_ds.idx2word[i], pred[0, i])\n","    print(train_ds.idx2word[i])\n","\n","print(\"\\ntarget\\n\")\n","for i in np.where(val_target[x] == 1)[0]:\n","    print(train_ds.idx2word[i])"],"execution_count":92,"outputs":[{"output_type":"stream","text":["pred\n","\n","top.n.01\n","man.n.01\n","group.n.01\n","people.n.01\n","field.n.01\n","woman.n.01\n","person.n.01\n","baseball.n.01\n","couple.n.01\n","slope.n.01\n","\n","target\n","\n","slope.n.01\n","person.n.01\n","snow.n.01\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J-pz9hCIbjAK"},"source":[""],"execution_count":null,"outputs":[]}]}